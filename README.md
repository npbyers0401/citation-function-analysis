# Citation Function Analysis

This repository contains code and data supporting the conclusions made in Byers et. al (CITATION).

## Requirements / Setup

### Install
Example setup using Anaconda:

Create your environment. 

`conda create -n myenv`

Activate your environment. 

`conda activate myenv`

Install pip if not already present.

`conda install pip`

Install required packages using *requirements.txt*.

`pip install -r requirements.txt`

Check installed packages if necessary.

`conda list`

### Environment Variables

This script assumes that two environment variables have been set:
* HF_HOME - local directory location storing your HuggingFace Hub cache and token data.
* HF_TOKEN - Hugging Face Hub User Access Token.

## Analysis Workflow

### Text Generation

The control function `run.py` initiates an iterative batch analysis for a set of data citations. Parameters for a given run are set via JSON configuration files stored in the `config` directory:
* `run_name` - string - Name of batch. Determines name of results sub-directory.
* `four_bit_quant` - boolean - Determines whether or not to quantize the chosen model to four-bit precision. If both `four_bit_quant` and `eight_bit_quant` are set to 'True', the script will choose four-bit precision.
* `eight_bit_quant` - boolean - Determines whether or not to quantize the chosen model to eight-bit precision. If both `four_bit_quant` and `eight_bit_quant` are set to 'True', the script will choose four-bit precision.
* `system_prompt` - string - Determines system prompt supplied to the model for all passes in the batch.
*  `key_pair_dir` - string - Directory path location of .csv file containing the paper>accession pairs constituting the current batch (i.e *"./data/key_pairs/training_pairs.csv"*)
*  `key_meta_dir` - string - Directory path location of .csv file containing the accession metadata for the accessions in the current batch (i.e *"./data/key_metadata/prompt_endpoint_static_INITIAL.csv"*)
*  `do_sample` - boolean - Determines whether or not to allow sampling for certain models.
*  `top_p` - int or null - Determines 'top p' value for certain models.
*  `temperature` - float - Determines temperature value for certain models.

Most of the text generation steps are accomplished using functions from `utils.py`.

Example usage:

`python run.py -c config/config_init0.json`

**Note that this script assumes that your environment has access to adequate GPU and CPU resources. Please check relevant specifications for a given model before running.**

### Outputs

The script generates three output files for each article>accession pair listed in the file specified by the config `key_pair_dir` value:
* RESULTS - Contains only the machine-generated values for 'Data Accessed', 'Use Cases', and 'Tools & Software' for a given article>accession pair.
* MESSAGE - Contains the final formatted chat conversation leading to the values in the RESULTS file.
* PARAMS - Contains each iterative message provided to the model during a chat session along with all selected parameters for each.

## Manual Evaluation

Once a batch is complete, manual evaluation is accomplished using a tabular file generated using `eval_document.py`. This script produces a .csv file containing all of the manual and machine-generated annotations for each article>accession pair. 

Values are assigned in the *score* column as follows:
* False Positive machine-generated value: -1
* True Positive machine-generated value: 1
* False Negative human-generated value: -1
* All others, including redundant values: 0

Refer to Byers, et al (CITATION) for specifics on aggregating redundant values.

Example usage:

`python eval_document.py -o data/results/val_0temp_meta_full_l405 -l data/labels/eval`

Parameters:
* `-o --outdir` : Directory containing output files from `run.py`. It should be *'data/results/{run_name}'*, with *'run_name'* set by your config file.
* `-l --labeldir` : Directory containing manual evaluation JSON files.

This script assumes that you have produced manual labels for all article>accession pairs. See existing examples in *'data/labels'* for formatting specifics.

## Scoring

Precision, Recall, and F1 scores can be generated from your evaluation file using `scoring.py`. This script assumes that all *'score'* values in your evaluation document are set to 0, 1, or -1.

Example usage:

`python scoring.py -e data/results/val_0temp_meta_full_l405/eval1_val_0temp_meta_full_l40.csv`

Parameters:
* `-e --evalpath` : Path to the manual evaluation .csv file generated by `eval_document.py`.


